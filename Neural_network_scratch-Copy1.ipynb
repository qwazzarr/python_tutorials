{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.360, loss: 1.099, lr: 0.0575\n",
      "epoch: 100, acc: 0.753, loss: 0.607, lr: 0.05749715382138749\n",
      "epoch: 200, acc: 0.787, loss: 0.479, lr: 0.057488558646061005\n",
      "epoch: 300, acc: 0.863, loss: 0.353, lr: 0.0574742170579969\n",
      "epoch: 400, acc: 0.893, loss: 0.293, lr: 0.05745413336430066\n",
      "epoch: 500, acc: 0.900, loss: 0.260, lr: 0.05742831359305091\n",
      "epoch: 600, acc: 0.920, loss: 0.230, lr: 0.05739676549028411\n",
      "epoch: 700, acc: 0.920, loss: 0.208, lr: 0.05735949851612153\n",
      "epoch: 800, acc: 0.930, loss: 0.201, lr: 0.05731652384004162\n",
      "epoch: 900, acc: 0.920, loss: 0.200, lr: 0.05726785433530062\n",
      "epoch: 1000, acc: 0.937, loss: 0.179, lr: 0.05721350457250595\n",
      "epoch: 1100, acc: 0.943, loss: 0.172, lr: 0.057153490812346115\n",
      "epoch: 1200, acc: 0.950, loss: 0.167, lr: 0.05708783099748289\n",
      "epoch: 1300, acc: 0.953, loss: 0.160, lr: 0.05701654474361081\n",
      "epoch: 1400, acc: 0.957, loss: 0.155, lr: 0.05693965332969104\n",
      "epoch: 1500, acc: 0.957, loss: 0.150, lr: 0.05685717968736526\n",
      "epoch: 1600, acc: 0.960, loss: 0.145, lr: 0.05676914838955814\n",
      "epoch: 1700, acc: 0.957, loss: 0.140, lr: 0.05667558563827575\n",
      "epoch: 1800, acc: 0.940, loss: 0.163, lr: 0.05657651925160783\n",
      "epoch: 1900, acc: 0.960, loss: 0.133, lr: 0.05647197864994457\n",
      "epoch: 2000, acc: 0.957, loss: 0.130, lr: 0.05636199484141552\n",
      "epoch: 2100, acc: 0.963, loss: 0.127, lr: 0.05624660040656211\n",
      "epoch: 2200, acc: 0.963, loss: 0.124, lr: 0.056125829482253836\n",
      "epoch: 2300, acc: 0.963, loss: 0.121, lr: 0.0559997177448593\n",
      "epoch: 2400, acc: 0.963, loss: 0.118, lr: 0.05586830239268352\n",
      "epoch: 2500, acc: 0.963, loss: 0.114, lr: 0.05573162212768421\n",
      "epoch: 2600, acc: 0.963, loss: 0.111, lr: 0.05558971713647895\n",
      "epoch: 2700, acc: 0.967, loss: 0.108, lr: 0.055442629070656384\n",
      "epoch: 2800, acc: 0.967, loss: 0.105, lr: 0.05529040102640576\n",
      "epoch: 2900, acc: 0.970, loss: 0.102, lr: 0.055133077523477216\n",
      "epoch: 3000, acc: 0.973, loss: 0.106, lr: 0.05497070448348897\n",
      "epoch: 3100, acc: 0.970, loss: 0.101, lr: 0.05480332920759486\n",
      "epoch: 3200, acc: 0.970, loss: 0.099, lr: 0.0546310003535278\n",
      "epoch: 3300, acc: 0.970, loss: 0.098, lr: 0.05445376791203501\n",
      "epoch: 3400, acc: 0.970, loss: 0.097, lr: 0.05427168318272092\n",
      "epoch: 3500, acc: 0.970, loss: 0.095, lr: 0.05408479874931414\n",
      "epoch: 3600, acc: 0.970, loss: 0.094, lr: 0.053893168454374266\n",
      "epoch: 3700, acc: 0.970, loss: 0.093, lr: 0.053696847373457236\n",
      "epoch: 3800, acc: 0.970, loss: 0.092, lr: 0.053495891788754996\n",
      "epoch: 3900, acc: 0.967, loss: 0.091, lr: 0.053290359162228046\n",
      "epoch: 4000, acc: 0.967, loss: 0.090, lr: 0.05308030810824858\n",
      "epoch: 4100, acc: 0.967, loss: 0.089, lr: 0.05286579836577227\n",
      "epoch: 4200, acc: 0.967, loss: 0.088, lr: 0.052646890770057186\n",
      "epoch: 4300, acc: 0.967, loss: 0.087, lr: 0.05242364722394871\n",
      "epoch: 4400, acc: 0.967, loss: 0.085, lr: 0.05219613066874968\n",
      "epoch: 4500, acc: 0.967, loss: 0.084, lr: 0.051964405054694185\n",
      "epoch: 4600, acc: 0.967, loss: 0.083, lr: 0.05172853531104512\n",
      "epoch: 4700, acc: 0.967, loss: 0.082, lr: 0.05148858731583416\n",
      "epoch: 4800, acc: 0.970, loss: 0.081, lr: 0.051244627865265006\n",
      "epoch: 4900, acc: 0.970, loss: 0.080, lr: 0.05099672464279867\n",
      "epoch: 5000, acc: 0.967, loss: 0.079, lr: 0.050744946187941495\n",
      "epoch: 5100, acc: 0.970, loss: 0.078, lr: 0.05048936186475566\n",
      "epoch: 5200, acc: 0.970, loss: 0.077, lr: 0.05023004183011235\n",
      "epoch: 5300, acc: 0.970, loss: 0.081, lr: 0.04996705700170821\n",
      "epoch: 5400, acc: 0.967, loss: 0.078, lr: 0.049700479025864754\n",
      "epoch: 5500, acc: 0.967, loss: 0.077, lr: 0.04943038024513185\n",
      "epoch: 5600, acc: 0.967, loss: 0.075, lr: 0.04915683366571511\n",
      "epoch: 5700, acc: 0.970, loss: 0.075, lr: 0.04887991292474815\n",
      "epoch: 5800, acc: 0.967, loss: 0.074, lr: 0.048599692257429626\n",
      "epoch: 5900, acc: 0.970, loss: 0.073, lr: 0.04831624646404558\n",
      "epoch: 6000, acc: 0.967, loss: 0.073, lr: 0.048029650876897874\n",
      "epoch: 6100, acc: 0.973, loss: 0.072, lr: 0.04773998132715866\n",
      "epoch: 6200, acc: 0.977, loss: 0.071, lr: 0.04744731411167093\n",
      "epoch: 6300, acc: 0.973, loss: 0.070, lr: 0.04715172595971584\n",
      "epoch: 6400, acc: 0.977, loss: 0.070, lr: 0.046853293999766794\n",
      "epoch: 6500, acc: 0.977, loss: 0.069, lr: 0.04655209572624976\n",
      "epoch: 6600, acc: 0.980, loss: 0.068, lr: 0.04624820896633035\n",
      "epoch: 6700, acc: 0.980, loss: 0.068, lr: 0.0459417118467466\n",
      "epoch: 6800, acc: 0.980, loss: 0.067, lr: 0.045632682760708115\n",
      "epoch: 6900, acc: 0.977, loss: 0.066, lr: 0.0453212003348793\n",
      "epoch: 7000, acc: 0.980, loss: 0.066, lr: 0.045007343396467496\n",
      "epoch: 7100, acc: 0.980, loss: 0.065, lr: 0.044691190940434077\n",
      "epoch: 7200, acc: 0.980, loss: 0.064, lr: 0.04437282209684748\n",
      "epoch: 7300, acc: 0.980, loss: 0.064, lr: 0.044052316098396604\n",
      "epoch: 7400, acc: 0.980, loss: 0.063, lr: 0.043729752248083044\n",
      "epoch: 7500, acc: 0.980, loss: 0.062, lr: 0.04340520988711025\n",
      "epoch: 7600, acc: 0.980, loss: 0.062, lr: 0.04307876836298711\n",
      "epoch: 7700, acc: 0.980, loss: 0.061, lr: 0.04275050699786355\n",
      "epoch: 7800, acc: 0.980, loss: 0.060, lr: 0.04242050505711567\n",
      "epoch: 7900, acc: 0.980, loss: 0.060, lr: 0.042088841718196685\n",
      "epoch: 8000, acc: 0.980, loss: 0.059, lr: 0.04175559603977091\n",
      "epoch: 8100, acc: 0.980, loss: 0.058, lr: 0.04142084693114665\n",
      "epoch: 8200, acc: 0.977, loss: 0.058, lr: 0.04108467312202396\n",
      "epoch: 8300, acc: 0.983, loss: 0.057, lr: 0.04074715313257311\n",
      "epoch: 8400, acc: 0.980, loss: 0.056, lr: 0.040408365243858754\n",
      "epoch: 8500, acc: 0.980, loss: 0.055, lr: 0.04006838746862444\n",
      "epoch: 8600, acc: 0.983, loss: 0.056, lr: 0.03972729752245274\n",
      "epoch: 8700, acc: 0.983, loss: 0.058, lr: 0.03938517279531407\n",
      "epoch: 8800, acc: 0.983, loss: 0.054, lr: 0.03904209032351884\n",
      "epoch: 8900, acc: 0.983, loss: 0.053, lr: 0.03869812676208572\n",
      "epoch: 9000, acc: 0.983, loss: 0.052, lr: 0.038353358357538996\n",
      "epoch: 9100, acc: 0.983, loss: 0.051, lr: 0.03800786092114778\n",
      "epoch: 9200, acc: 0.983, loss: 0.051, lr: 0.03766170980261899\n",
      "epoch: 9300, acc: 0.977, loss: 0.051, lr: 0.03731497986425555\n",
      "epoch: 9400, acc: 0.983, loss: 0.049, lr: 0.03696774545559145\n",
      "epoch: 9500, acc: 0.983, loss: 0.050, lr: 0.03662008038851448\n",
      "epoch: 9600, acc: 0.983, loss: 0.048, lr: 0.0362720579128865\n",
      "epoch: 9700, acc: 0.983, loss: 0.048, lr: 0.03592375069267149\n",
      "epoch: 9800, acc: 0.983, loss: 0.047, lr: 0.03557523078258122\n",
      "epoch: 9900, acc: 0.983, loss: 0.047, lr: 0.03522656960524691\n",
      "epoch: 10000, acc: 0.987, loss: 0.048, lr: 0.0348778379289257\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "# Our sample dataset\n",
    "def create_data(n, k):\n",
    "    X = np.zeros((n*k, 2))  # data matrix (each row = single example)\n",
    "    y = np.zeros(n*k, dtype='uint8')  # class labels\n",
    "    for j in range(k):\n",
    "        ix = range(n*j, n*(j+1))\n",
    "        r = np.linspace(0.0, 1, n)  # radius\n",
    "        t = np.linspace(j*4, (j+1)*4, n) + np.random.randn(n)*0.2  # theta\n",
    "        X[ix] = np.c_[r*np.sin(t*2.5), r*np.cos(t*2.5)]\n",
    "        y[ix] = j\n",
    "    return X, y\n",
    "\n",
    "\n",
    "# Dense layer\n",
    "class Layer_Dense:\n",
    "\n",
    "    # Layer initialization\n",
    "    def __init__(self, inputs, neurons):\n",
    "        # Initialize weights and biases\n",
    "        self.weights = 0.01 * np.random.randn(inputs, neurons)\n",
    "        self.biases = np.zeros((1, neurons))\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Calculate output values from input ones, weights and biases\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Gradients on parameters\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        # Gradient on values\n",
    "        self.dvalues = np.dot(dvalues, self.weights.T)\n",
    "\n",
    "\n",
    "# ReLU activation\n",
    "class Activation_ReLU:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Calculate output values from input ones\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Since we need to modify original variable, \n",
    "        # let's make a copy of values first\n",
    "        self.dvalues = dvalues.copy()\n",
    "\n",
    "        # Zero gradient where input values were negative \n",
    "        self.dvalues[self.inputs <= 0] = 0 \n",
    "\n",
    "\n",
    "# Softmax activation\n",
    "class Activation_Softmax:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "\n",
    "        # get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        # normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "\n",
    "        self.output = probabilities\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        self.dvalues = dvalues.copy()\n",
    "\n",
    "\n",
    "# Cross-entropy loss\n",
    "class Loss_CategoricalCrossentropy:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "\n",
    "        # Number of samples in a batch\n",
    "        samples = y_pred.shape[0]\n",
    "\n",
    "        # Probabilities for target values - only if categorical labels\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_pred = y_pred[range(samples), y_true]\n",
    "\n",
    "        # Losses\n",
    "        negative_log_likelihoods = -np.log(y_pred)\n",
    "\n",
    "        # Mask values - only for one-hot encoded labels\n",
    "        if len(y_true.shape) == 2:\n",
    "            negative_log_likelihoods *= y_true\n",
    "\n",
    "        # Overall loss\n",
    "        data_loss = np.sum(negative_log_likelihoods) / samples\n",
    "        return data_loss\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "\n",
    "        samples = dvalues.shape[0]\n",
    "\n",
    "        self.dvalues = dvalues.copy()  # Copy so we can safely modify\n",
    "        self.dvalues[range(samples), y_true] -= 1\n",
    "        self.dvalues = self.dvalues / samples\n",
    "class Optimizer_Adam:\n",
    "\n",
    "    # Initialize optimizer - set settings\n",
    "    def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7, beta_1=0.9, beta_2=0.999):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.current_learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "\n",
    "        # If layer does not contain cache arrays,\n",
    "        # create them filled with zeros\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        # Update momentum  with current gradients\n",
    "        layer.weight_momentums = (self.beta_1 * layer.weight_momentums +\n",
    "                                 (1 - self.beta_1) * layer.dweights)\n",
    "        layer.bias_momentums = (self.beta_1 * layer.bias_momentums +\n",
    "                               (1 - self.beta_1) * layer.dbiases)\n",
    "        # Get corrected momentum\n",
    "        # self.iteration is 0 at first pass\n",
    "        # and we need to start with 1 here\n",
    "        weight_momentums_corrected = (layer.weight_momentums /\n",
    "            (1 - self.beta_1 ** (self.iterations + 1)))\n",
    "        bias_momentums_corrected = (layer.bias_momentums /\n",
    "            (1 - self.beta_1 ** (self.iterations + 1)))\n",
    "        # Update cache with squared current gradients\n",
    "        layer.weight_cache = (self.beta_2 * layer.weight_cache +\n",
    "            (1 - self.beta_2) * layer.dweights**2)\n",
    "        layer.bias_cache = (self.beta_2 * layer.bias_cache +\n",
    "            (1 - self.beta_2) * layer.dbiases**2)\n",
    "        # Get corrected cachebias\n",
    "        weight_cache_corrected = (layer.weight_cache /\n",
    "            (1 - self.beta_2 ** (self.iterations + 1)))\n",
    "        bias_cache_corrected = (layer.bias_cache /\n",
    "            (1 - self.beta_2 ** (self.iterations + 1)))\n",
    "\n",
    "        # Vanilla SGD parameter update + normalization with square rooted cache\n",
    "        layer.weights += (-self.current_learning_rate *\n",
    "                         weight_momentums_corrected /\n",
    "                         (np.sqrt(weight_cache_corrected) +\n",
    "                             self.epsilon))\n",
    "        layer.biases += (-self.current_learning_rate *\n",
    "                         bias_momentums_corrected /\n",
    "                         (np.sqrt(bias_cache_corrected) +\n",
    "                             self.epsilon))\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "\n",
    "# Create dataset\n",
    "X, y = create_data(100, 3)\n",
    "\n",
    "# Create Dense layer with 2 input features and 3 output values\n",
    "dense1 = Layer_Dense(2, 64)  # first dense layer, 2 inputs (each sample has 2 features), 3 outputs\n",
    "\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Create second Dense layer with 3 input features (as we take output of previous layer here) and 3 output values (output values)\n",
    "dense2 = Layer_Dense(64, 3)  # second dense layer, 3 inputs, 3 outputs\n",
    "\n",
    "# Create Softmax activation (to be used with Dense layer):\n",
    "activation2 = Activation_Softmax()\n",
    "\n",
    "# Create loss function\n",
    "loss_function = Loss_CategoricalCrossentropy()\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = Optimizer_Adam(learning_rate=0.0575, decay=1e-8)\n",
    "\n",
    "# Train in loop\n",
    "for epoch in range(10001):\n",
    "\n",
    "    # Make a forward pass of our training data thru this layer\n",
    "    dense1.forward(X)\n",
    "\n",
    "    # Make a forward pass thru activation function - we take output of previous layer here\n",
    "    activation1.forward(dense1.output)\n",
    "\n",
    "    # Make a forward pass thru second Dense layer - it takes outputs of activation function of first layer as inputs\n",
    "    dense2.forward(activation1.output)\n",
    "\n",
    "    # Make a forward pass thru activation function - we take output of previous layer here\n",
    "    activation2.forward(dense2.output)\n",
    "\n",
    "    # Calculate loss from output of activation2 so softmax activation\n",
    "    loss = loss_function.forward(activation2.output, y)\n",
    "\n",
    "    # Calculate accuracy from output of activation2 and targets\n",
    "    predictions = np.argmax(activation2.output, axis=1)  # calculate values along first axis\n",
    "    accuracy = np.mean(predictions==y)\n",
    "\n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, acc: {accuracy:.3f}, loss: {loss:.3f}, lr: {optimizer.current_learning_rate}')\n",
    "\n",
    "    # Backward pass\n",
    "    loss_function.backward(activation2.output, y)\n",
    "    activation2.backward(loss_function.dvalues)\n",
    "    dense2.backward(activation2.dvalues)\n",
    "    activation1.backward(dense2.dvalues)\n",
    "    dense1.backward(activation1.dvalues)\n",
    "\n",
    "    # Update weights\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()\n",
    "\n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
